{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pos_tagger_assignment.ipynb","provenance":[],"authorship_tag":"ABX9TyNUpABB2mI+94ltR1iuB2+3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ahLid8zZW5km","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlHtU-oSc7aE","colab_type":"code","outputId":"80079b5c-70ec-42c3-e895-39a6c3ffee6b","executionInfo":{"status":"ok","timestamp":1591947358228,"user_tz":-330,"elapsed":36220,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpfzJNqShKxm","colab_type":"code","outputId":"aad7c2dc-95ea-49b0-8387-ca27d275694f","executionInfo":{"status":"ok","timestamp":1591947627865,"user_tz":-330,"elapsed":161431,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}},"colab":{"base_uri":"https://localhost:8080/","height":325}},"source":["% pip install pomegranate\n","#Pomegranate is a graphical models library for Python, implemented in Cython for speed"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting pomegranate\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/8a/51bb4268722c26f67738a0da8ab43df49bbb01016a135b6aa1c45bd33670/pomegranate-0.13.3.tar.gz (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 3.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pomegranate) (1.18.5)\n","Requirement already satisfied: joblib>=0.9.0b4 in /usr/local/lib/python3.6/dist-packages (from pomegranate) (0.15.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from pomegranate) (2.4)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pomegranate) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from pomegranate) (3.13)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->pomegranate) (4.4.2)\n","Building wheels for collected packages: pomegranate\n","  Building wheel for pomegranate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pomegranate: filename=pomegranate-0.13.3-cp36-cp36m-linux_x86_64.whl size=10950038 sha256=894656f6091b41748942700350a0b841eab015fd1db28446967f7b1751f4f2eb\n","  Stored in directory: /root/.cache/pip/wheels/92/a4/39/0f9e71a9134d03801477bffa6e02257020f12e5cbb031a8489\n","Successfully built pomegranate\n","Installing collected packages: pomegranate\n","Successfully installed pomegranate-0.13.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DlWFWn6FiMio","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append(\"/content/drive/My Drive/Colab Notebooks/assignment\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkmiEtoWizEd","colab_type":"code","colab":{}},"source":["from IPython.core.display import HTML\n","from itertools import chain\n","from collections import Counter,defaultdict\n","#from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rymcx5XX6Pcf","colab_type":"code","colab":{}},"source":["from helpers import show_model, Dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yRTtNMA6MlC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"fe7c3667-bbb4-43ba-e635-0b607cdf3881","executionInfo":{"status":"ok","timestamp":1591948529724,"user_tz":-330,"elapsed":4453,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["data = Dataset(\"/content/drive/My Drive/Colab Notebooks/assignment/tags-universal.txt\", \"/content/drive/My Drive/Colab Notebooks/assignment/brown-universal.txt\", train_test_split=0.8)\n","\n","print(\"There are {} sentences in the corpus.\".format(len(data)))\n","print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n","print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n","\n","assert len(data) == len(data.training_set) + len(data.testing_set), \\\n","       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""],"execution_count":19,"outputs":[{"output_type":"stream","text":["There are 57340 sentences in the corpus.\n","There are 45872 sentences in the training set.\n","There are 11468 sentences in the testing set.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_PWud9OEqH_V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"904db51f-5efc-4e49-8301-ae784d22742b","executionInfo":{"status":"ok","timestamp":1591948533586,"user_tz":-330,"elapsed":1061,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["key = 'b100-38532'\n","print(\"Sentence: {}\".format(key))\n","print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n","print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Sentence: b100-38532\n","words:\n","\t('Perhaps', 'it', 'was', 'right', ';', ';')\n","tags:\n","\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fqf5t72oqTEk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"5973938b-584d-4d1d-b164-186677084a7a","executionInfo":{"status":"ok","timestamp":1591950362555,"user_tz":-330,"elapsed":1086,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["print(\"There are a total of {} samples of {} unique words in the corpus.\"\n","      .format(data.N, len(data.vocab)))\n","print(\"There are {} samples of {} unique words in the training set.\"\n","      .format(data.training_set.N, len(data.training_set.vocab)))\n","print(\"There are {} samples of {} unique words in the testing set.\"\n","      .format(data.testing_set.N, len(data.testing_set.vocab)))\n","print(\"There are {} words in the test set that are missing in the training set.\"\n","      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n","\n","assert data.N == data.training_set.N + data.testing_set.N, \\\n","       \"The number of training + test samples should sum to the total number of samples\""],"execution_count":21,"outputs":[{"output_type":"stream","text":["There are a total of 1161192 samples of 56057 unique words in the corpus.\n","There are 928458 samples of 50536 unique words in the training set.\n","There are 232734 samples of 25112 unique words in the testing set.\n","There are 5521 words in the test set that are missing in the training set.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Smt33C45qp9h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":397},"outputId":"acb7f0ff-3208-40be-f34d-97e16bf1529e","executionInfo":{"status":"ok","timestamp":1591950388152,"user_tz":-330,"elapsed":945,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["# accessing words with Dataset.X and tags with Dataset.Y \n","for i in range(5):    \n","    print(\"Sentence {}:\".format(i + 1), data.X[i])\n","    print()\n","    print(\"Labels {}:\".format(i + 1), data.Y[i])\n","    print()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n","\n","Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n","\n","Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n","\n","Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n","\n","Sentence 3: ('Such', 'an', 'instrument', 'is', 'expected', 'to', 'be', 'especially', 'useful', 'if', 'it', 'could', 'be', 'used', 'to', 'measure', 'the', 'elasticity', 'of', 'heavy', 'pastes', 'such', 'as', 'printing', 'inks', ',', 'paints', ',', 'adhesives', ',', 'molten', 'plastics', ',', 'and', 'bread', 'dough', ',', 'for', 'the', 'elasticity', 'is', 'related', 'to', 'those', 'various', 'properties', 'termed', '``', 'length', \"''\", ',', '``', 'shortness', \"''\", ',', '``', 'spinnability', \"''\", ',', 'etc.', ',', 'which', 'are', 'usually', 'judged', 'by', 'subjective', 'methods', 'at', 'present', '.')\n","\n","Labels 3: ('PRT', 'DET', 'NOUN', 'VERB', 'VERB', 'PRT', 'VERB', 'ADV', 'ADJ', 'ADP', 'PRON', 'VERB', 'VERB', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ', 'ADP', 'VERB', 'NOUN', '.', 'NOUN', '.', 'NOUN', '.', 'ADJ', 'NOUN', '.', 'CONJ', 'NOUN', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', '.', 'NOUN', '.', '.', '.', 'NOUN', '.', '.', '.', 'NOUN', '.', '.', 'ADV', '.', 'DET', 'VERB', 'ADV', 'VERB', 'ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', '.')\n","\n","Sentence 4: ('My', 'future', 'plans', 'are', 'to', 'become', 'a', 'language', 'teacher', '.')\n","\n","Labels 4: ('DET', 'ADJ', 'NOUN', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'NOUN', '.')\n","\n","Sentence 5: ('We', 'ran', 'east', 'for', 'about', 'half', 'a', 'mile', 'before', 'we', 'turned', 'back', 'to', 'the', 'road', ',', 'panting', 'from', 'the', 'effort', 'and', 'soaked', 'with', 'sweat', '.')\n","\n","Labels 5: ('PRON', 'VERB', 'NOUN', 'ADP', 'ADV', 'PRT', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', '.', 'VERB', 'ADP', 'DET', 'NOUN', 'CONJ', 'VERB', 'ADP', 'NOUN', '.')\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1pd719Acquke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":287},"outputId":"036ef0bd-fb1c-489e-e95d-91e6bdaf83c9","executionInfo":{"status":"ok","timestamp":1591950420834,"user_tz":-330,"elapsed":977,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["# use Dataset.stream() (word, tag) samples for the entire corpus\n","print(\"\\nStream (word, tag) pairs:\\n\")\n","for i, pair in enumerate(data.stream()):\n","    print(\"\\t\", pair)\n","    if i > 10: break"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","Stream (word, tag) pairs:\n","\n","\t ('Mr.', 'NOUN')\n","\t ('Podger', 'NOUN')\n","\t ('had', 'VERB')\n","\t ('thanked', 'VERB')\n","\t ('him', 'PRON')\n","\t ('gravely', 'ADV')\n","\t (',', '.')\n","\t ('and', 'CONJ')\n","\t ('now', 'ADV')\n","\t ('he', 'PRON')\n","\t ('made', 'VERB')\n","\t ('use', 'NOUN')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D44Wj-4mtamg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3ffbf182-09b3-4b7c-c451-11ed4648a4ac","executionInfo":{"status":"ok","timestamp":1591950467610,"user_tz":-330,"elapsed":1221,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["from collections import Counter,defaultdict\n","\n","def pair_counts(tags,words):\n","\td=defaultdict(lambda: defaultdict(int))\n","\tfor tag,word in zip(tags,words):\n","\t\td[tag][word]+=1\n","\treturn d\n","\traise NotImplementedError\n","tags = [tag for i,(word, tag) in enumerate(data.training_set.stream())]\n","words = [word for i,(word, tag) in enumerate(data.training_set.stream())]\n","\n","emission_counts =pair_counts(tags,words)\n","\n","tags"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ADV',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," '.',\n"," 'VERB',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," 'CONJ',\n"," 'VERB',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'VERB',\n"," 'ADJ',\n"," 'PRT',\n"," 'ADP',\n"," 'NUM',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'VERB',\n"," 'NOUN',\n"," 'NUM',\n"," '.',\n"," 'NUM',\n"," '.',\n"," '.',\n"," '.',\n"," 'ADP',\n"," 'ADV',\n"," 'NUM',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'CONJ',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'CONJ',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," '.',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADV',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'DET',\n"," 'NOUN',\n"," 'CONJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADJ',\n"," '.',\n"," 'NOUN',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'VERB',\n"," 'NOUN',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NUM',\n"," '.',\n"," 'NUM',\n"," '.',\n"," 'PRT',\n"," 'DET',\n"," 'NOUN',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADV',\n"," 'CONJ',\n"," 'VERB',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'NOUN',\n"," 'VERB',\n"," 'DET',\n"," '.',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'ADV',\n"," 'ADV',\n"," 'ADJ',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'CONJ',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'ADP',\n"," 'NOUN',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'ADV',\n"," 'PRT',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'VERB',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADV',\n"," 'ADJ',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'VERB',\n"," '.',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'PRT',\n"," 'DET',\n"," 'NOUN',\n"," 'CONJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADJ',\n"," 'ADJ',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'VERB',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'ADJ',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADJ',\n"," 'ADV',\n"," 'PRT',\n"," 'VERB',\n"," 'PRON',\n"," '.',\n"," 'VERB',\n"," 'ADV',\n"," 'ADV',\n"," 'ADJ',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'PRON',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'NOUN',\n"," 'DET',\n"," 'ADV',\n"," 'VERB',\n"," 'NOUN',\n"," 'CONJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'ADV',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADV',\n"," 'VERB',\n"," 'CONJ',\n"," 'VERB',\n"," 'VERB',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," 'VERB',\n"," 'DET',\n"," 'VERB',\n"," 'NOUN',\n"," 'ADV',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," '.',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," '.',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'ADJ',\n"," 'NOUN',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," 'VERB',\n"," 'NUM',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'VERB',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," 'ADP',\n"," 'NUM',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," 'ADJ',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'PRON',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'ADJ',\n"," 'ADP',\n"," 'PRT',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'ADJ',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," 'ADV',\n"," '.',\n"," 'PRT',\n"," 'NUM',\n"," 'NUM',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," '.',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'CONJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'NUM',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'ADP',\n"," 'PRT',\n"," 'VERB',\n"," 'NUM',\n"," 'NOUN',\n"," 'NOUN',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," '.',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'PRON',\n"," '.',\n"," 'CONJ',\n"," 'ADV',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'PRON',\n"," 'PRON',\n"," 'ADV',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'DET',\n"," 'NUM',\n"," 'NOUN',\n"," 'PRON',\n"," 'ADV',\n"," 'VERB',\n"," 'ADP',\n"," 'PRON',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADJ',\n"," 'ADV',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'ADV',\n"," 'VERB',\n"," 'VERB',\n"," 'PRON',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," 'ADJ',\n"," 'ADP',\n"," 'VERB',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'DET',\n"," 'PRON',\n"," 'VERB',\n"," 'ADV',\n"," 'VERB',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADV',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADJ',\n"," '.',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'ADJ',\n"," 'ADP',\n"," 'DET',\n"," 'NUM',\n"," 'NOUN',\n"," 'VERB',\n"," 'PRON',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADV',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'VERB',\n"," 'NOUN',\n"," 'ADP',\n"," 'NUM',\n"," 'VERB',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'ADV',\n"," 'NUM',\n"," 'NUM',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'PRON',\n"," '.',\n"," 'ADP',\n"," 'ADJ',\n"," 'NOUN',\n"," 'CONJ',\n"," 'ADJ',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," 'ADJ',\n"," 'PRT',\n"," 'VERB',\n"," 'NUM',\n"," 'ADP',\n"," 'NUM',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," 'VERB',\n"," '.',\n"," 'ADP',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'PRT',\n"," 'VERB',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'PRT',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADV',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'CONJ',\n"," 'VERB',\n"," 'PRT',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'NOUN',\n"," 'NOUN',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'VERB',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'PRT',\n"," 'VERB',\n"," 'NOUN',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," '.',\n"," 'ADV',\n"," '.',\n"," 'VERB',\n"," 'ADV',\n"," 'VERB',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'ADP',\n"," 'DET',\n"," 'VERB',\n"," 'NUM',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," '.',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," 'VERB',\n"," 'PRT',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'PRON',\n"," 'VERB',\n"," '.',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'ADV',\n"," 'ADV',\n"," 'DET',\n"," 'ADJ',\n"," 'NOUN',\n"," 'VERB',\n"," 'VERB',\n"," 'ADV',\n"," 'PRON',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'CONJ',\n"," 'VERB',\n"," 'ADV',\n"," '.',\n"," 'VERB',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'ADJ',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'VERB',\n"," 'NOUN',\n"," 'CONJ',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," 'PRT',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," 'PRON',\n"," 'ADV',\n"," 'VERB',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'CONJ',\n"," 'PRON',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'PRON',\n"," 'NOUN',\n"," 'VERB',\n"," 'DET',\n"," 'NUM',\n"," 'NUM',\n"," '.',\n"," '.',\n"," 'PRON',\n"," 'VERB',\n"," 'PRON',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," 'ADV',\n"," '.',\n"," '.',\n"," 'CONJ',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADV',\n"," 'ADP',\n"," 'NOUN',\n"," 'DET',\n"," 'VERB',\n"," 'ADV',\n"," 'VERB',\n"," 'PRT',\n"," 'VERB',\n"," 'ADJ',\n"," 'NOUN',\n"," 'ADP',\n"," 'DET',\n"," 'ADJ',\n"," 'CONJ',\n"," 'ADJ',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," '.',\n"," 'PRT',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'PRT',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'VERB',\n"," 'DET',\n"," 'NOUN',\n"," 'ADP',\n"," 'ADJ',\n"," 'VERB',\n"," 'ADP',\n"," 'DET',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," 'ADP',\n"," 'NOUN',\n"," '.',\n"," 'DET',\n"," 'NOUN',\n"," 'VERB',\n"," 'ADP',\n"," 'NOUN',\n"," 'NOUN',\n"," '.',\n"," 'NOUN',\n"," '.',\n"," ...]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"dcwmg-Vtt-uB","colab_type":"code","colab":{}},"source":["# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n","from collections import namedtuple\n","\n","FakeState = namedtuple(\"FakeState\", \"name\")\n","\n","class MFCTagger:\n","    # NOTE: You should not need to modify this class or any of its methods\n","    missing = FakeState(name=\"<MISSING>\")\n","    \n","    def __init__(self, table):\n","        self.table = defaultdict(lambda: MFCTagger.missing)\n","        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n","        \n","    def viterbi(self, seq):\n","        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n","        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n","\n","\n","# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n","# the same as the emission probabilities) and use it to fill the mfc_table\n","\n","tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n","words = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n","#Since this is the word_counts we will pass first words and then counts\n","word_counts = pair_counts(words,tags)\n","\n","mfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())\n","\n","# DO NOT MODIFY BELOW THIS LINE\n","mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkdFAyV4wEAL","colab_type":"code","colab":{}},"source":["def replace_unknown(sequence):\n","    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n","    by the literal string value 'nan'. Pomegranate will ignore these values\n","    during computation.\n","    \"\"\"\n","    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n","\n","def simplify_decoding(X, model):\n","    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n","    _, state_path = model.viterbi(replace_unknown(X))\n","    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lppovC3vwU74","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2b070cf5-afd6-4093-8a9c-54efab86d480","executionInfo":{"status":"ok","timestamp":1591950594362,"user_tz":-330,"elapsed":937,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["for key in data.testing_set.keys[:5]:\n","    print(\"Sentence Key: {}\\n\".format(key))\n","    print(\"Predicted labels:\\n-----------------\")\n","    print(simplify_decoding(data.sentences[key].words, mfc_model))\n","    print()\n","    print(\"Actual labels:\\n--------------\")\n","    print(data.sentences[key].tags)\n","    print(\"\\n\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Sentence Key: b100-28144\n","\n","Predicted labels:\n","-----------------\n","['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n","\n","Actual labels:\n","--------------\n","('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n","\n","\n","Sentence Key: b100-23146\n","\n","Predicted labels:\n","-----------------\n","['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n","\n","Actual labels:\n","--------------\n","('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n","\n","\n","Sentence Key: b100-35462\n","\n","Predicted labels:\n","-----------------\n","['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n","\n","Actual labels:\n","--------------\n","('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n","\n","\n","Sentence Key: b100-37008\n","\n","Predicted labels:\n","-----------------\n","['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n","\n","Actual labels:\n","--------------\n","('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', '.')\n","\n","\n","Sentence Key: b100-18135\n","\n","Predicted labels:\n","-----------------\n","['CONJ', '<MISSING>', 'VERB', 'VERB', '.']\n","\n","Actual labels:\n","--------------\n","('CONJ', 'NOUN', 'VERB', 'VERB', '.')\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FoRqyZmPw866","colab_type":"code","colab":{}},"source":["def accuracy(X, Y, model):\n"," \n","    correct = total_predictions = 0\n","    for observations, actual_tags in zip(X, Y):\n","        \n","        # The model.viterbi call in simplify_decoding will return None if the HMM\n","        # raises an error (for example, if a test sentence contains a word that\n","        # is out of vocabulary for the training set). Any exception counts the\n","        # full sentence as an error (which makes this a conservative estimate).\n","        try:\n","            most_likely_tags = simplify_decoding(observations, model)\n","            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n","        except:\n","            pass\n","        total_predictions += len(observations)\n","    return correct / total_predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7A7iqWmxlMp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"a2bb2629-9d99-491b-b40e-b6de62f8158f","executionInfo":{"status":"ok","timestamp":1591950634875,"user_tz":-330,"elapsed":2150,"user":{"displayName":"dinto davi t","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBzTy2uxCzWU5xvgWj48ddZNjW4kQHA-5edAlwaA=s64","userId":"01449815747933117428"}}},"source":["mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n","print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n","\n","mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n","print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["training accuracy mfc_model: 95.72%\n","testing accuracy mfc_model: 93.01%\n"],"name":"stdout"}]}]}
